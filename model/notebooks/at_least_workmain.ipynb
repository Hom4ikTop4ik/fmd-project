{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import normalizer\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import cv2\n",
    "\n",
    "import models\n",
    "import modelutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, yhat, y):\n",
    "        return torch.sqrt(self.mse(yhat, y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = data_loader.load(\"I:/NSU/CV/tests/torch/data/train/coords\",\n",
    "                         \"I:/NSU/CV/tests/torch/data/train/images\", \n",
    "                        firstn = 6000, batchSize = 16, shuffle = True, \n",
    "                        displace = True, size = 400, show = False)\n",
    "\n",
    "\n",
    "\n",
    "scaler = normalizer.MinMaxNormalizer()\n",
    "scaler.fit([y for _, y in train])\n",
    "\n",
    "print(\"Number of batches:\", len(train))\n",
    "for x, y in train:\n",
    "   print(x.shape, y.shape)\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"devise is: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouth_pointlist = [44, 7, 33, 14, 2, 31, 49, 15, 42, 32, 9, 51, 38, 61,\n",
    "    18, 23, 12, 47, 67, 1, 2]\n",
    "mouth_boundaries = [7, 14, 15, 67]\n",
    "eye_L_pointlist = [62, 65, 0, 13, 34, 64]\n",
    "eye_R_pointlist = [16, 36, 54, 55, 53, 63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coords_to_img(coordsbatch, idlist_list, imgsize, blobsize, bglevel):\n",
    "    imglist = []\n",
    "    for coords in coordsbatch:\n",
    "        true_cord_list = []\n",
    "        for idlist in idlist_list:\n",
    "            midx = 0.0\n",
    "            midy = 0.0\n",
    "            for id in idlist:\n",
    "                midx += coords[id][0]\n",
    "                midy += coords[id][1]\n",
    "            midx /= len(idlist)\n",
    "            midy /= len(idlist)\n",
    "            img = np.zeros((imgsize, imgsize), dtype=np.float32)\n",
    "            x = int(midx * imgsize)\n",
    "            y = int(midy * imgsize)\n",
    "            true_cord_list.append(np.array([x, y]))\n",
    "\n",
    "        for cord in true_cord_list:\n",
    "            img[cord[1], cord[0]] = 15 * blobsize\n",
    "        \n",
    "        img = cv2.GaussianBlur(img, (151, 151), sigmaX=blobsize, borderType=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        for cord in true_cord_list:\n",
    "            cv2.circle(img, (cord[0], cord[1]), 1, 6, -1)\n",
    "        \n",
    "    # img[y, x] = 15000\n",
    "    # img = cv2.GaussianBlur(img, (351, 351), sigmaX=50, borderType=cv2.BORDER_REPLICATE)\n",
    "    # cv2.circle(img, (x, y), 0, 6, -1)\n",
    "    # img = cv2.GaussianBlur(img, (11, 11), sigmaX=1)\n",
    "\n",
    "        img = cv2.GaussianBlur(img, (11, 11), sigmaX=2)\n",
    "        img += bglevel\n",
    "        imglist.append(torch.from_numpy(img).unsqueeze(0))\n",
    "    return torch.stack(imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_coords(landmarks_list, tensor):\n",
    "    x = torch.zeros(tensor.shape[0]).to(device)\n",
    "    y = torch.zeros(tensor.shape[0]).to(device)\n",
    "    for id in landmarks_list:\n",
    "        x += tensor[:,id,0]\n",
    "        y += tensor[:,id,1]\n",
    "    x /= len(landmarks_list)\n",
    "    y /= len(landmarks_list)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMSELoss(nn.Module):\n",
    "    def __init__(self,  device, l2_lambda = 0.0):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "    def forward(self, x, y,weightmap, parameters = None):\n",
    "        ls = ((x-y) * weightmap)**2\n",
    "        sm = ls.sum() / (x.shape[0] * x.shape[1])\n",
    "        if parameters is None:\n",
    "            return sm\n",
    "        \n",
    "        pk = 0.0\n",
    "        smp = 0.0\n",
    "        if parameters is not None:\n",
    "            for param in parameters:\n",
    "                smp += (param**2).mean()\n",
    "                pk += 1\n",
    "\n",
    "        return sm + (smp / float(pk)) * self.l2_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDetector(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CnnDetector, self).__init__()\n",
    "        self.adpool = nn.AdaptiveAvgPool2d((128, 128)).to(device)\n",
    "        self.pool = nn.MaxPool2d(2, 2).to(device)\n",
    "        self.conv1 = nn.Conv2d(3, 3, 7, padding = 3).to(device) \n",
    "        self.conv2 = nn.Conv2d(3, 3, 3, padding = 1).to(device)\n",
    "        self.conv3 = nn.Conv2d(3, 3, 3, padding = 1).to(device)\n",
    "        self.conv4 = nn.Conv2d(3, 3, 3, padding = 1).to(device)\n",
    "        self.conv5 = nn.Conv2d(3, 3, 3, padding = 1).to(device)\n",
    "        self.conv6 = nn.Conv2d(3, 3, 3, padding = 1).to(device)\n",
    "        self.conv7 = nn.Conv2d(3, 3, 3, padding = 1).to(device)\n",
    "        self.conv8 = nn.Conv2d(3, 1, 3, padding = 1).to(device)\n",
    "        self.act = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, H, W]\n",
    "        x = self.adpool(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.pool(self.act(self.conv2(x)))\n",
    "        # # x1 = x.clone()\n",
    "        x = self.act(self.conv3(x))\n",
    "        # # x1 = x.clone()\n",
    "        x = self.act(self.conv4(x)) # add skip connections (+x.copy()) if needed\n",
    "        x = self.act(self.conv5(x))\n",
    "        x = self.act(self.conv6(x)) # x1 = x.clone()\n",
    "        # x = self.act(self.conv7(x))\n",
    "        # x = self.act(self.conv8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnDetector(device).to(device)\n",
    "\n",
    "criterion = WMSELoss(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19, Loss: 30.70862, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.76786, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.42286, midweigth 0.006930931005626917\n",
      "385.18933 6.1318e-40 8.86438 37.64441\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 31.19982, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 31.12621, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 30.36101, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.27616, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.33024, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.36479, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.40930, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.45334, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.19213, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 30.23978, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.69023, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 30.46231, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.74839, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.41495, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.24659, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 1, Loss: 30.50520\n",
      "Batch 19, Loss: 30.48503, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.27649, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.75202, midweigth 0.006930931005626917\n",
      "385.18933 0.0 8.864502 37.644535\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.48931, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 30.38561, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 30.20049, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.60146, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.11470, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.12651, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.57685, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.45775, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.39866, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 30.35927, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.45589, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 29.87024, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.17283, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.45515, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.36366, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 2, Loss: 30.36692\n",
      "Batch 19, Loss: 29.98158, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.82590, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.33478, midweigth 0.006930931005626917\n",
      "385.18933 3.690873e-38 8.864418 37.643387\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.27102, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 30.64983, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 30.28346, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.79293, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.69671, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.89679, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.37655, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.52518, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.12841, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 30.49776, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.35181, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 29.99296, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.28256, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.34303, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.11691, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 3, Loss: 30.39065\n",
      "Batch 19, Loss: 30.38939, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 29.72309, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.43248, midweigth 0.006930931005626917\n",
      "385.1894 0.0 8.864417 37.64702\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.11140, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 30.01050, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 30.17773, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.37352, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.32325, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.02811, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.17452, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.53933, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.22932, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 30.18152, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.10407, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 30.49036, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.50034, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.18618, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.34055, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 4, Loss: 30.24678\n",
      "Batch 19, Loss: 30.22433, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.03067, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.09159, midweigth 0.006930931005626917\n",
      "385.18936 5.673407e-36 8.864418 37.64693\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.11601, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 29.90736, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 30.11584, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.38835, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.27271, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.73783, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.14092, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.46219, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.31541, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 29.81611, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.01440, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 29.88513, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 29.59146, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.13261, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.32417, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 5, Loss: 30.13282\n",
      "Batch 19, Loss: 30.09611, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 29.82241, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.09948, midweigth 0.006930931005626917\n",
      "385.18933 2.942979e-39 8.864418 37.64455\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.05436, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 29.92919, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 29.85586, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.33062, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.12956, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 29.94258, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.03988, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.62296, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.25438, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 29.61076, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.20855, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 29.93822, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.40308, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 29.84233, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.31192, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 6, Loss: 30.09164\n",
      "Batch 19, Loss: 29.83137, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.28467, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 30.22700, midweigth 0.006930931005626917\n",
      "385.18958 1.1131579e-38 8.864382 37.651924\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.01020, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 30.22703, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 29.90429, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 30.31744, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 30.14834, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.05990, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.05214, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.40916, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.24900, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 29.60603, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 29.50500, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 29.65105, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.03211, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.01777, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 30.16776, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 7, Loss: 30.03510\n",
      "Batch 19, Loss: 30.14139, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.13728, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 29.58864, midweigth 0.006930931005626917\n",
      "385.18936 1.390611e-35 8.864382 37.646683\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 30.88582, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 30.12736, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 29.87276, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 29.74581, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 29.63962, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 30.67753, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 30.22337, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.01296, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 29.98116, midweigth 0.006930931005626917\n",
      "Batch 259, Loss: 29.45063, midweigth 0.006930931005626917\n",
      "Batch 279, Loss: 30.08020, midweigth 0.006930931005626917\n",
      "Batch 299, Loss: 29.96417, midweigth 0.006930931005626917\n",
      "Batch 319, Loss: 30.00763, midweigth 0.006930931005626917\n",
      "Batch 339, Loss: 30.05053, midweigth 0.006930931005626917\n",
      "Batch 359, Loss: 29.80016, midweigth 0.006930931005626917\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 8, Loss: 30.01179\n",
      "Batch 19, Loss: 30.18455, midweigth 0.006930931005626917\n",
      "Batch 39, Loss: 30.38273, midweigth 0.006930931005626917\n",
      "Batch 59, Loss: 29.79215, midweigth 0.006930931005626917\n",
      "385.18933 9.84558e-38 8.864502 37.643387\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 29.82691, midweigth 0.006930931005626917\n",
      "Batch 99, Loss: 30.13849, midweigth 0.006930931005626917\n",
      "Batch 119, Loss: 30.19030, midweigth 0.006930931005626917\n",
      "Batch 139, Loss: 29.97744, midweigth 0.006930931005626917\n",
      "Batch 159, Loss: 29.96933, midweigth 0.006930931005626917\n",
      "Batch 179, Loss: 29.90185, midweigth 0.006930931005626917\n",
      "Batch 199, Loss: 29.99149, midweigth 0.006930931005626917\n",
      "Batch 219, Loss: 30.32919, midweigth 0.006930931005626917\n",
      "Batch 239, Loss: 30.04576, midweigth 0.006930931005626917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     18\u001b[0m imganswers \u001b[38;5;241m=\u001b[39m coords_to_img(answers, [eye_L_pointlist, eye_R_pointlist], \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, bglevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 19\u001b[0m weightmap \u001b[38;5;241m=\u001b[39m \u001b[43mcoords_to_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43meye_L_pointlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meye_R_pointlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbglevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(needshow):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#xshow = inputs[0].cpu().numpy()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     yshow \u001b[38;5;241m=\u001b[39m (outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[194], line 15\u001b[0m, in \u001b[0;36mcoords_to_img\u001b[1;34m(coordsbatch, idlist_list, imgsize, blobsize, bglevel)\u001b[0m\n\u001b[0;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((imgsize, imgsize), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(midx \u001b[38;5;241m*\u001b[39m imgsize)\n\u001b[1;32m---> 15\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmidy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimgsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     true_cord_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([x, y]))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cord \u001b[38;5;129;01min\u001b[39;00m true_cord_list:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "# learning loop\n",
    "epoch_loss = 0\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    step_loss = 0\n",
    "    step_count = 0\n",
    "    random.shuffle(train)\n",
    "    for batch_idx, (inputs, answers) in enumerate(train):\n",
    "        needshow = torch.tensor(False).to(device)\n",
    "        if(batch_idx % 350 == 60):\n",
    "           needshow = True\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        answers = answers.to(device)\n",
    "        outputs = model(inputs)\n",
    "        imganswers = coords_to_img(answers, [eye_L_pointlist, eye_R_pointlist], 64, 5, bglevel = 0).to(device)\n",
    "        weightmap = coords_to_img(answers, [eye_L_pointlist, eye_R_pointlist], 64, 10, bglevel = 0).to(device)\n",
    "        if(needshow):\n",
    "            #xshow = inputs[0].cpu().numpy()\n",
    "            yshow = (outputs[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            ansshow = (imganswers[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            xshow = (inputs[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            weightshow = (weightmap[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            print(ansshow.max(), ansshow.min(), ansshow.mean(), ansshow.std())\n",
    "            print(type(ansshow[0][0]))\n",
    "            yshow = (yshow - yshow.min()) * 255 / (yshow.max() - yshow.min())\n",
    "            ansshow = ansshow * 255 / ansshow.max()\n",
    "            xshow = xshow * 255 / xshow.max()\n",
    "            weightshow = weightshow * 255 / weightshow.max()\n",
    "            pil_mod = Image.fromarray(yshow)\n",
    "            pil_mod.show()\n",
    "            # print(yshow.max(), yshow.min(), yshow.mean(), yshow.std())\n",
    "            # pil_imagey = Image.fromarray(ansshow)\n",
    "            # pil_imagey.show()\n",
    "            # pil_weighshow = Image.fromarray(weightshow)\n",
    "            # pil_weighshow.show()\n",
    "            \n",
    "\n",
    "        loss = criterion(outputs, imganswers, weightmap)\n",
    "        step_loss += loss\n",
    "        step_count += 1\n",
    "        if(step_count == 20):\n",
    "            optimizer.zero_grad()\n",
    "            step_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Batch {batch_idx}, Loss: {step_loss.item() / step_count:.5f}, midweigth {model.conv7.weight.detach().cpu().numpy().mean()}')\n",
    "            step_count = 0\n",
    "            step_loss = 0\n",
    "\n",
    "        epoch_loss += loss.item() \n",
    "        \n",
    "\n",
    "        if batch_idx == 1600:\n",
    "            break\n",
    "    \n",
    "    #learning_rate /= 10.0\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f'shape {inputs.shape}, Epoch {epoch + 1}, Loss: {epoch_loss/len(train):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testloop\n",
    "criterion = SelectiveRMSELoss(eye_L_pointlist + eye_R_pointlist, False, device)\n",
    "epoch_loss = 0\n",
    "test = data_loader.load(\"I:/NSU/CV/tests/torch/data/test/coords\",\n",
    "                        \"I:/NSU/CV/tests/torch/data/test/images\",  \n",
    "                        firstn = 2000, batchSize = 16, shuffle = True, displace = True, \n",
    "                        size = 400, show = False)\n",
    "random.shuffle(test)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, answers) in enumerate(test):\n",
    "        inputs = inputs.to(device)\n",
    "        answers = answers.to(device)\n",
    "        answers = scaler.transform(answers)\n",
    "        outputs = model(inputs)\n",
    "        outputs = scaler.inverse_transform(outputs)\n",
    "        loss = criterion(outputs, answers)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {epoch_loss/len(test):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(test)\n",
    "with torch.no_grad():\n",
    "    inputs, answers = test[1]\n",
    "    inputs = inputs.to(device)\n",
    "    answers = answers.to(device)\n",
    "    outputs = model(inputs)\n",
    "    outputs = scaler.inverse_transform(outputs)\n",
    "    # x_l, y_l = get_mean_coords(eye_L_pointlist, outputs)\n",
    "    # x_r, y_r = get_mean_coords(eye_R_pointlist, outputs)\n",
    "    # print(x_l[0], y_l[0], x_r[0], y_r[0])\n",
    "    # outputs[0][1][0] = x_l[0]\n",
    "    # outputs[0][1][1] = y_l[0]\n",
    "    # outputs[0][2][0] = x_r[0]\n",
    "    # outputs[0][2][1] = y_r[0]\n",
    "    print(outputs.shape, answers.shape)\n",
    "    img = modelutils.show_tensor(inputs[0], outputs[0])\n",
    "    img.show()\n",
    "    imgdlib = modelutils.show_tensor(inputs[0], answers[0])\n",
    "    imgdlib.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Save the model after training\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, 'globalDetector.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mouthBoundDetector(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(mouthBoundDetector, self).__init__()\n",
    "        self.last_detector_size = 128 \n",
    "        self.adpool = nn.AdaptiveAvgPool2d((128, 128)).to(device) \n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, padding = 1).to(device)\n",
    "        self.pool = nn.MaxPool2d(2, 2).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, 9, 3, padding = 1).to(device)\n",
    "        self.conv3 = nn.Conv2d(9, 20, 3, padding = 1).to(device)\n",
    "        self.conv4 = nn.Conv2d(20, self.last_detector_size, 3, padding = 1).to(device)\n",
    "        fcsize = 256\n",
    "        self.fc1 = nn.Linear(self.last_detector_size*16*16, fcsize).to(device)\n",
    "        self.fc_list = []\n",
    "        for i in range(3):\n",
    "            self.fc_list.append(nn.Linear(fcsize, fcsize).to(device))\n",
    "        self.prelast = nn.Linear(fcsize, fcsize).to(device)\n",
    "        self.fc_last = nn.Linear(fcsize, 3 * 68).to(device)\n",
    "        self.act = nn.ReLU().to(device)\n",
    "        self.sigm = nn.Sigmoid().to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, H, W]\n",
    "        x = self.adpool(x)\n",
    "        x = self.pool(self.act(self.conv1(x)))\n",
    "        x = self.pool(self.act(self.conv2(x)))\n",
    "        x = self.act(self.conv3(x))  \n",
    "        x = self.pool(self.act(self.conv4(x)))\n",
    "        x = x.view(-1, self.last_detector_size*16*16)       \n",
    "        x = self.act(self.fc1(x))\n",
    "        for i in range(len(self.fc_list)):\n",
    "            x = self.act(self.fc_list[i](x))\n",
    "        x = self.act(self.prelast(x))\n",
    "        x = self.fc_last(x)\n",
    "        x = x.view(-1, 68, 3)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, answers = test[1]\n",
    "    inputs = inputs.to(device)\n",
    "    answers = answers.to(device)\n",
    "    outputs = model(inputs)\n",
    "    outputs = scaler.inverse_transform(outputs)\n",
    "    print(outputs.shape, answers.shape)\n",
    "    img = show_tensor(inputs[0], outputs[0])\n",
    "    img.show()\n",
    "    imgdlib = show_tensor(inputs[0], answers[0])\n",
    "    imgdlib.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
