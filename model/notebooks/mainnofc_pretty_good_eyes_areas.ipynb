{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import normalizer\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import cv2\n",
    "\n",
    "import models\n",
    "import modelutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, yhat, y):\n",
    "        return torch.sqrt(self.mse(yhat, y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = data_loader.load(\"I:/NSU/CV/tests/torch/data/train/coords\",\n",
    "                         \"I:/NSU/CV/tests/torch/data/train/images\", \n",
    "                        firstn = 6000, batchSize = 16, shuffle = True, \n",
    "                        displace = True, size = 400, show = False)\n",
    "\n",
    "\n",
    "\n",
    "scaler = normalizer.MinMaxNormalizer()\n",
    "scaler.fit([y for _, y in train])\n",
    "\n",
    "print(\"Number of batches:\", len(train))\n",
    "for x, y in train:\n",
    "   print(x.shape, y.shape)\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"devise is: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouth_pointlist = [44, 7, 33, 14, 2, 31, 49, 15, 42, 32, 9, 51, 38, 61,\n",
    "    18, 23, 12, 47, 67, 1, 2]\n",
    "mouth_boundaries = [7, 14, 15, 67]\n",
    "eye_L_pointlist = [62, 65, 0, 13, 34, 64]\n",
    "eye_R_pointlist = [16, 36, 54, 55, 53, 63]\n",
    "\n",
    "eyes_corners = [53,36,62,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coords_to_img(coordsbatch, idlist_list, imgsize, blobsize, bglevel):\n",
    "    imglist = []\n",
    "    for coords in coordsbatch:\n",
    "        true_cord_list = []\n",
    "        for idlist in idlist_list:\n",
    "            midx = 0.0\n",
    "            midy = 0.0\n",
    "            for id in idlist:\n",
    "                midx += coords[id][0]\n",
    "                midy += coords[id][1]\n",
    "            midx /= len(idlist)\n",
    "            midy /= len(idlist)\n",
    "            img = np.zeros((imgsize, imgsize), dtype=np.float32)\n",
    "            x = int(midx * imgsize)\n",
    "            y = int(midy * imgsize)\n",
    "            true_cord_list.append(np.array([x, y]))\n",
    "\n",
    "        for cord in true_cord_list:\n",
    "            img[cord[1], cord[0]] = 15 * blobsize\n",
    "        \n",
    "        img = cv2.GaussianBlur(img, (151, 151), sigmaX=blobsize, borderType=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        for cord in true_cord_list:\n",
    "            cv2.circle(img, (cord[0], cord[1]), 1, 6, -1)\n",
    "        \n",
    "    # img[y, x] = 15000\n",
    "    # img = cv2.GaussianBlur(img, (351, 351), sigmaX=50, borderType=cv2.BORDER_REPLICATE)\n",
    "    # cv2.circle(img, (x, y), 0, 6, -1)\n",
    "    # img = cv2.GaussianBlur(img, (11, 11), sigmaX=1)\n",
    "\n",
    "        img = cv2.GaussianBlur(img, (11, 11), sigmaX=2)\n",
    "        img += bglevel\n",
    "        imglist.append(torch.from_numpy(img).unsqueeze(0))\n",
    "    return torch.stack(imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_coords(landmarks_list, tensor):\n",
    "    x = torch.zeros(tensor.shape[0]).to(device)\n",
    "    y = torch.zeros(tensor.shape[0]).to(device)\n",
    "    for id in landmarks_list:\n",
    "        x += tensor[:,id,0]\n",
    "        y += tensor[:,id,1]\n",
    "    x /= len(landmarks_list)\n",
    "    y /= len(landmarks_list)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMSELoss(nn.Module):\n",
    "    def __init__(self,  device, l2_lambda = 0.0):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "    def forward(self, x, y,weightmap, parameters = None):\n",
    "        ls = ((x-y) * weightmap)**2\n",
    "        sm = ls.sum() / (x.shape[0] * x.shape[1])\n",
    "        if parameters is None:\n",
    "            return sm\n",
    "        \n",
    "        pk = 0.0\n",
    "        smp = 0.0\n",
    "        if parameters is not None:\n",
    "            for param in parameters:\n",
    "                smp += (param**2).mean()\n",
    "                pk += 1\n",
    "\n",
    "        return sm + (smp / float(pk)) * self.l2_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDetector(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CnnDetector, self).__init__()\n",
    "        self.adpool = nn.AdaptiveAvgPool2d((128, 128)).to(device)\n",
    "        self.pool = nn.MaxPool2d(2, 2).to(device)\n",
    "        self.mid_depth = mid_depth = 7\n",
    "        self.conv1 = nn.Conv2d(3, mid_depth, 7, padding = 3).to(device) \n",
    "        self.conv2 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv3 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv4 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv5 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv6 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv7 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv8 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv9 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv10 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv11 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv12 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv13 = nn.Conv2d(mid_depth, mid_depth, 3, padding = 1).to(device)\n",
    "        self.conv14 = nn.Conv2d(mid_depth, 1, 3, padding = 1).to(device)\n",
    "        self.act = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, H, W]\n",
    "        x = self.adpool(x)\n",
    "        x1 = x.clone()\n",
    "        x1 = x1.repeat(1, 3, 1, 1)[:, :self.mid_depth, :, :]\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.pool(self.act(self.conv2(x)) + x1)\n",
    "        x1 = x.clone()\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.act(self.conv4(x)) + x1 # add skip connections (+x.copy()) if needed\n",
    "        x1 = x.clone()\n",
    "        x = self.act(self.conv5(x))\n",
    "        x = self.act(self.conv6(x)) + x1 \n",
    "        x1 = x.clone()\n",
    "        x = self.act(self.conv7(x))\n",
    "        # x = self.act(self.conv8(x)) + x1\n",
    "        # x1 = x.clone()\n",
    "        # x = self.act(self.conv9(x))\n",
    "        # x = self.act(self.conv10(x)) + x1\n",
    "        # x1 = x.clone()\n",
    "        # x = self.act(self.conv11(x))\n",
    "        # x = self.act(self.conv12(x)) + x1\n",
    "        # x1 = x.clone()\n",
    "        # x = self.act(self.conv13(x))\n",
    "        # x = self.act(self.conv14(x))\n",
    "        \n",
    "        x = x1 * 0.3 + self.act(self.conv9(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnDetector(device).to(device)\n",
    "\n",
    "criterion = WMSELoss(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstolsofls(listoflist):\n",
    "    newlist = []\n",
    "    for listt in listoflist:\n",
    "        newlist.append([listt])\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19, Loss: 56.77617, midweigth 0.0001637489622225985\n",
      "Batch 39, Loss: 58.17774, midweigth 0.0013590239686891437\n",
      "Batch 59, Loss: 59.54799, midweigth -0.0014799181371927261\n",
      "352.39722 2.3355422e-08 14.695499 36.9263\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 56.46287, midweigth -0.00383376469835639\n",
      "Batch 99, Loss: 56.14426, midweigth -0.005699213594198227\n",
      "Batch 119, Loss: 55.79831, midweigth -0.007298904005438089\n",
      "Batch 139, Loss: 55.23645, midweigth -0.008699263446033001\n",
      "Batch 159, Loss: 55.59592, midweigth -0.009937863796949387\n",
      "Batch 179, Loss: 54.99949, midweigth -0.011040893383324146\n",
      "Batch 199, Loss: 54.41158, midweigth -0.012026926502585411\n",
      "Batch 219, Loss: 53.64951, midweigth -0.012911097146570683\n",
      "Batch 239, Loss: 53.69338, midweigth -0.013706330209970474\n",
      "Batch 259, Loss: 52.44177, midweigth -0.014423406682908535\n",
      "Batch 279, Loss: 52.30703, midweigth -0.015071353875100613\n",
      "Batch 299, Loss: 51.43046, midweigth -0.0156578179448843\n",
      "Batch 319, Loss: 51.05738, midweigth -0.016189370304346085\n",
      "Batch 339, Loss: 50.36388, midweigth -0.01667170599102974\n",
      "Batch 359, Loss: 48.80859, midweigth -0.01710979826748371\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 1, Loss: 54.04891\n",
      "Batch 19, Loss: 48.70424, midweigth -0.01750801131129265\n",
      "Batch 39, Loss: 47.72869, midweigth -0.017870210111141205\n",
      "Batch 59, Loss: 46.15757, midweigth -0.018199825659394264\n",
      "352.74045 3.112909e-08 14.694971 37.20649\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 44.11979, midweigth -0.01849992200732231\n",
      "Batch 99, Loss: 44.25942, midweigth -0.018773239105939865\n",
      "Batch 119, Loss: 43.58578, midweigth -0.01902221329510212\n",
      "Batch 139, Loss: 43.16649, midweigth -0.019249066710472107\n",
      "Batch 159, Loss: 39.37114, midweigth -0.0194558035582304\n",
      "Batch 179, Loss: 44.31359, midweigth -0.019644249230623245\n",
      "Batch 199, Loss: 39.59618, midweigth -0.019816016778349876\n",
      "Batch 219, Loss: 43.12099, midweigth -0.019972577691078186\n",
      "Batch 239, Loss: 42.28339, midweigth -0.020115280523896217\n",
      "Batch 259, Loss: 38.03836, midweigth -0.020245369523763657\n",
      "Batch 279, Loss: 39.29398, midweigth -0.02036399394273758\n",
      "Batch 299, Loss: 40.79994, midweigth -0.020472176373004913\n",
      "Batch 319, Loss: 36.13284, midweigth -0.020570795983076096\n",
      "Batch 339, Loss: 38.09272, midweigth -0.02066069468855858\n",
      "Batch 359, Loss: 37.80778, midweigth -0.020742636173963547\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 2, Loss: 41.73398\n",
      "Batch 19, Loss: 34.27828, midweigth -0.020817330107092857\n",
      "Batch 39, Loss: 35.35340, midweigth -0.020885448902845383\n",
      "Batch 59, Loss: 35.29436, midweigth -0.02094755321741104\n",
      "352.53036 1.3317448e-07 14.695347 37.050537\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 34.56870, midweigth -0.02100415714085102\n",
      "Batch 99, Loss: 35.65908, midweigth -0.02105574682354927\n",
      "Batch 119, Loss: 34.56214, midweigth -0.021102754399180412\n",
      "Batch 139, Loss: 33.67465, midweigth -0.021145589649677277\n",
      "Batch 159, Loss: 33.52849, midweigth -0.02118464931845665\n",
      "Batch 179, Loss: 33.16592, midweigth -0.021220244467258453\n",
      "Batch 199, Loss: 32.99119, midweigth -0.021252669394016266\n",
      "Batch 219, Loss: 32.82634, midweigth -0.02128220535814762\n",
      "Batch 239, Loss: 33.05002, midweigth -0.021309101954102516\n",
      "Batch 259, Loss: 32.98377, midweigth -0.0213336031883955\n",
      "Batch 279, Loss: 33.18557, midweigth -0.021355951204895973\n",
      "Batch 299, Loss: 31.88974, midweigth -0.02137630619108677\n",
      "Batch 319, Loss: 32.28760, midweigth -0.021394852548837662\n",
      "Batch 339, Loss: 31.60646, midweigth -0.021411744877696037\n",
      "Batch 359, Loss: 31.79823, midweigth -0.02142714336514473\n",
      "shape torch.Size([16, 3, 400, 400]), Epoch 3, Loss: 33.38339\n",
      "Batch 19, Loss: 31.71735, midweigth -0.02144119329750538\n",
      "Batch 39, Loss: 31.80115, midweigth -0.021454021334648132\n",
      "Batch 59, Loss: 30.39738, midweigth -0.0214657261967659\n",
      "353.06818 1.2097913e-10 14.66197 37.42031\n",
      "<class 'numpy.float32'>\n",
      "Batch 79, Loss: 31.31251, midweigth -0.021476391702890396\n",
      "Batch 99, Loss: 30.96435, midweigth -0.02148611657321453\n",
      "Batch 119, Loss: 30.98230, midweigth -0.021494990214705467\n",
      "Batch 139, Loss: 31.40421, midweigth -0.021503102034330368\n",
      "Batch 159, Loss: 29.72426, midweigth -0.02151051163673401\n",
      "Batch 179, Loss: 29.94994, midweigth -0.02151728793978691\n",
      "Batch 199, Loss: 28.72190, midweigth -0.021523479372262955\n",
      "Batch 219, Loss: 30.07680, midweigth -0.021529166027903557\n",
      "Batch 239, Loss: 29.75441, midweigth -0.02153436839580536\n",
      "Batch 259, Loss: 30.00447, midweigth -0.021539149805903435\n",
      "Batch 279, Loss: 29.17803, midweigth -0.021543538197875023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[327], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     18\u001b[0m imganswers \u001b[38;5;241m=\u001b[39m coords_to_img(answers, [eye_L_pointlist, eye_R_pointlist], \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m6\u001b[39m, bglevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 19\u001b[0m weightmap \u001b[38;5;241m=\u001b[39m \u001b[43mcoords_to_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43meye_L_pointlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43meye_R_pointlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmouth_pointlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbglevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(needshow):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#xshow = inputs[0].cpu().numpy()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     yshow \u001b[38;5;241m=\u001b[39m (outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[194], line 21\u001b[0m, in \u001b[0;36mcoords_to_img\u001b[1;34m(coordsbatch, idlist_list, imgsize, blobsize, bglevel)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cord \u001b[38;5;129;01min\u001b[39;00m true_cord_list:\n\u001b[0;32m     19\u001b[0m     img[cord[\u001b[38;5;241m1\u001b[39m], cord[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;241m*\u001b[39m blobsize\n\u001b[1;32m---> 21\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGaussianBlur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m151\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m151\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmaX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblobsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mborderType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBORDER_REPLICATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cord \u001b[38;5;129;01min\u001b[39;00m true_cord_list:\n\u001b[0;32m     24\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mcircle(img, (cord[\u001b[38;5;241m0\u001b[39m], cord[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "# learning loop\n",
    "epoch_loss = 0\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    step_loss = 0\n",
    "    step_count = 0\n",
    "    random.shuffle(train)\n",
    "    for batch_idx, (inputs, answers) in enumerate(train):\n",
    "        needshow = torch.tensor(False).to(device)\n",
    "        if(batch_idx % 350 == 60):\n",
    "           needshow = True\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        answers = answers.to(device)\n",
    "        outputs = model(inputs)\n",
    "        imganswers = coords_to_img(answers, [eye_L_pointlist, eye_R_pointlist], 64, 6, bglevel = 0).to(device)\n",
    "        weightmap = coords_to_img(answers, [eye_L_pointlist,eye_R_pointlist, mouth_pointlist], 64, 15, bglevel = 0.05).to(device)\n",
    "        if(needshow):\n",
    "            #xshow = inputs[0].cpu().numpy()\n",
    "            yshow = (outputs[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            ansshow = (imganswers[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            xshow = (inputs[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            weightshow = (weightmap[0][0].cpu().detach().numpy() * 255.0).astype('float32')\n",
    "            print(ansshow.max(), ansshow.min(), ansshow.mean(), ansshow.std())\n",
    "            print(type(ansshow[0][0]))\n",
    "            yshow = (yshow - yshow.min()) * 255 / (yshow.max() - yshow.min())\n",
    "            ansshow = ansshow * 255 / ansshow.max()\n",
    "            xshow = xshow * 255 / xshow.max()\n",
    "            weightshow = weightshow * 255 / weightshow.max()\n",
    "            pil_mod = Image.fromarray(yshow)\n",
    "            pil_mod.show()\n",
    "            # print(yshow.max(), yshow.min(), yshow.mean(), yshow.std())\n",
    "            # pil_imagey = Image.fromarray(ansshow)\n",
    "            # pil_imagey.show()\n",
    "            # pil_weighshow = Image.fromarray(weightshow)\n",
    "            # pil_weighshow.show()\n",
    "            \n",
    "\n",
    "        loss = criterion(outputs, imganswers, weightmap)\n",
    "        step_loss += loss\n",
    "        step_count += 1\n",
    "        if(step_count == 20):\n",
    "            optimizer.zero_grad()\n",
    "            step_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Batch {batch_idx}, Loss: {step_loss.item() / step_count:.5f}, midweigth {model.conv7.weight.detach().cpu().numpy().mean()}')\n",
    "            step_count = 0\n",
    "            step_loss = 0\n",
    "\n",
    "        epoch_loss += loss.item() \n",
    "        \n",
    "\n",
    "        if batch_idx == 1600:\n",
    "            break\n",
    "    \n",
    "    #learning_rate /= 10.0\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f'shape {inputs.shape}, Epoch {epoch + 1}, Loss: {epoch_loss/len(train):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testloop\n",
    "criterion = SelectiveRMSELoss(eye_L_pointlist + eye_R_pointlist, False, device)\n",
    "epoch_loss = 0\n",
    "test = data_loader.load(\"I:/NSU/CV/tests/torch/data/test/coords\",\n",
    "                        \"I:/NSU/CV/tests/torch/data/test/images\",  \n",
    "                        firstn = 2000, batchSize = 16, shuffle = True, displace = True, \n",
    "                        size = 400, show = False)\n",
    "random.shuffle(test)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, answers) in enumerate(test):\n",
    "        inputs = inputs.to(device)\n",
    "        answers = answers.to(device)\n",
    "        answers = scaler.transform(answers)\n",
    "        outputs = model(inputs)\n",
    "        outputs = scaler.inverse_transform(outputs)\n",
    "        loss = criterion(outputs, answers)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Test Loss: {epoch_loss/len(test):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(test)\n",
    "with torch.no_grad():\n",
    "    inputs, answers = test[1]\n",
    "    inputs = inputs.to(device)\n",
    "    answers = answers.to(device)\n",
    "    outputs = model(inputs)\n",
    "    outputs = scaler.inverse_transform(outputs)\n",
    "    # x_l, y_l = get_mean_coords(eye_L_pointlist, outputs)\n",
    "    # x_r, y_r = get_mean_coords(eye_R_pointlist, outputs)\n",
    "    # print(x_l[0], y_l[0], x_r[0], y_r[0])\n",
    "    # outputs[0][1][0] = x_l[0]\n",
    "    # outputs[0][1][1] = y_l[0]\n",
    "    # outputs[0][2][0] = x_r[0]\n",
    "    # outputs[0][2][1] = y_r[0]\n",
    "    print(outputs.shape, answers.shape)\n",
    "    img = modelutils.show_tensor(inputs[0], outputs[0])\n",
    "    img.show()\n",
    "    imgdlib = modelutils.show_tensor(inputs[0], answers[0])\n",
    "    imgdlib.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Save the model after training\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, 'globalDetector.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mouthBoundDetector(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(mouthBoundDetector, self).__init__()\n",
    "        self.last_detector_size = 128 \n",
    "        self.adpool = nn.AdaptiveAvgPool2d((128, 128)).to(device) \n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, padding = 1).to(device)\n",
    "        self.pool = nn.MaxPool2d(2, 2).to(device)\n",
    "        self.conv2 = nn.Conv2d(6, 9, 3, padding = 1).to(device)\n",
    "        self.conv3 = nn.Conv2d(9, 20, 3, padding = 1).to(device)\n",
    "        self.conv4 = nn.Conv2d(20, self.last_detector_size, 3, padding = 1).to(device)\n",
    "        fcsize = 256\n",
    "        self.fc1 = nn.Linear(self.last_detector_size*16*16, fcsize).to(device)\n",
    "        self.fc_list = []\n",
    "        for i in range(3):\n",
    "            self.fc_list.append(nn.Linear(fcsize, fcsize).to(device))\n",
    "        self.prelast = nn.Linear(fcsize, fcsize).to(device)\n",
    "        self.fc_last = nn.Linear(fcsize, 3 * 68).to(device)\n",
    "        self.act = nn.ReLU().to(device)\n",
    "        self.sigm = nn.Sigmoid().to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 3, H, W]\n",
    "        x = self.adpool(x)\n",
    "        x = self.pool(self.act(self.conv1(x)))\n",
    "        x = self.pool(self.act(self.conv2(x)))\n",
    "        x = self.act(self.conv3(x))  \n",
    "        x = self.pool(self.act(self.conv4(x)))\n",
    "        x = x.view(-1, self.last_detector_size*16*16)       \n",
    "        x = self.act(self.fc1(x))\n",
    "        for i in range(len(self.fc_list)):\n",
    "            x = self.act(self.fc_list[i](x))\n",
    "        x = self.act(self.prelast(x))\n",
    "        x = self.fc_last(x)\n",
    "        x = x.view(-1, 68, 3)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, answers = test[1]\n",
    "    inputs = inputs.to(device)\n",
    "    answers = answers.to(device)\n",
    "    outputs = model(inputs)\n",
    "    outputs = scaler.inverse_transform(outputs)\n",
    "    print(outputs.shape, answers.shape)\n",
    "    img = show_tensor(inputs[0], outputs[0])\n",
    "    img.show()\n",
    "    imgdlib = show_tensor(inputs[0], answers[0])\n",
    "    imgdlib.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
